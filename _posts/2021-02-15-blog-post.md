---
title: 'Extending Imagenet models to 3-Dimensional Data'
date: 2021-02-15
permalink: /posts/2021/02/Imagenet-in-3D/
tags:
  - Deep Learning
  - Computer Vision
  - Pytorch
---

The Problem
======
One of the major obstacles faced in medical imaging is the sparsity of annotated, or labeled, data.  This can make it difficult to balance the need for deep learning models to train long enough to be useful while avoiding overfitting to the training data.  One of the easiest ways to address this is to give your model a headstart using transfer learning, which means initializing the model with parameters that were useful for another task before starting the training process.

Limitations of Transfer Learning
------
For many machine learning tasks, transfer learning handily solves several problems accompanying training on small datasets, but it imposes some strict, but straightforward, limitations on your model.  The first of these is that, in order to load transfer learned parameters into your model, the pretrained parameters must be compatible with the layer they are being loaded into.  For most practical purposes this means a significant portion of your model must have the same architecture as the model you are taking your parameters from.  The second is that your data must be fairly similar to the data the parameters were trained on, at least in regards to its overall dimensionality and the number of input channels.

ImageNet
------
[ImageNet](http://www.image-net.org/), a huge dataset of labeled "natural images" (e.g. standard color photographs), provides one of the computer vision community's favorite classification tasks to test the mettle of novel computer vision models.  Thanks to its easy availability and large size using ImageNet allows you to avoid the many issues that come with small datasets, and most widely used sets of pretrained parameters for computer vision models have been trained using ImageNet.

However, ImageNet can be difficult to apply to medical imaging.  This isn't because ImageNet is distinctly lacking any medical imaging, the parameters that result are often surprisingly generalizable, but because every image is a 2-dimensional, 3-channel (RGB) image while medical imaging (at least for CT data, which has been the focus of my work) is 3-dimensional, greyscale data.

Solutions for Medical Imaging
======
There are several ways one might resolve the problems that come with using ImageNet for training models to analyze medical images.  The simplest is to "convert" your data to ImageNet data.  You reduce the images to 2-dimensions (taking, for instance, a single slice from a CT scan), copy the greyscale data into 3 channels (for some machine learning frameworks this is automatic), and scale it to match the range used for the pretrained weights you've decided to use (generally either scaling pixel/voxel values to fall between 0 and 1 or normalizing by [ImageNet statistics](https://pytorch.org/vision/0.8/models.html)).  You have a little flexibility here, despite recommendations against it I generally scale my segmentation data to fall between -1 and 1 and get good results.

The problems start when you decide, for any of a number of reasons, that you'd like to take advantage of your data's third dimension.  One solution is to use that channel dimension we ignored in the 2-d case, using 3 adjacent slices as the 3 channels you input to your model, which often provides satisfactory results.

My Solution (Resnet Example)
------

