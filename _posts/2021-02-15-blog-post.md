---
title: 'Extending ImageNet models to 3-Dimensional Data'
date: 2021-02-15
permalink: /posts/2021/02/ImageNet-in-3D/
tags:
  - Deep Learning
  - Computer Vision
  - PyTorch
---

The Problem
======
One of the major obstacles faced in medical imaging is the sparsity of annotated, or labeled, data.  This can make it difficult to balance the need for deep learning models to train long enough to be useful while avoiding overfitting to the training data.  One of the easiest ways to address this is to give your model a head-start using transfer learning, which means initializing the model with parameters that were useful for another task before starting the training process.

Limitations of Transfer Learning
------
For many machine learning tasks, transfer learning handily solves several problems accompanying training on small datasets, but it imposes some strict, but straightforward, limitations on your model.  The first of these is that, in order to load transfer learned parameters into your model, the pretrained parameters must be compatible with the layer they are being loaded into.  For most practical purposes this means a significant portion of your model must have the same architecture as the model you are taking your parameters from.  The second is that your data must be fairly similar to the data the parameters were trained on, at least in regards to its overall dimensionality and the number of input channels.

ImageNet
======
[ImageNet](http://www.image-net.org/), a huge dataset of labeled "natural images" (e.g. standard color photographs), provides one of the computer vision community's favorite classification tasks to test the mettle of novel computer vision models.  Thanks to its easy availability and large size using ImageNet allows you to avoid the many issues that come with small datasets, and most widely used sets of pretrained parameters for computer vision models have been trained using ImageNet.

However, ImageNet can be difficult to apply to medical imaging.  This isn't because ImageNet is distinctly lacking any medical imaging, the parameters that result are often surprisingly generalizable, but because every image is a 2-dimensional, 3-channel (RGB) image while medical imaging (at least for CT data, which has been the focus of my work) is 3-dimensional, greyscale data.

Solutions for Medical Imaging
------
There are several ways one might resolve the problems that come with using ImageNet for training models to analyze medical images.  The simplest is to "convert" your data to ImageNet data.  You reduce the images to 2-dimensions (taking, for instance, a single slice from a CT scan), copy the greyscale data into 3 channels (for some machine learning frameworks this is automatic), and scale it to match the range used for the pretrained weights you've decided to use (generally either scaling pixel/voxel values to fall between 0 and 1 or normalizing by [ImageNet statistics](https://pytorch.org/vision/0.8/models.html)).  You have a little flexibility here, despite recommendations against it I generally scale my segmentation data to fall between -1 and 1 and get good results.

The problems start when you decide, for any of a number of reasons, that you'd like to take advantage of your data's third dimension.  One solution is to use that channel dimension we ignored in the 2-d case, using 3 adjacent slices as the 3 channels you input to your model, which often provides satisfactory results.

Another Solution ([ResNet Example](https://github.com/JDSobek/PublicMLCode/tree/main/3dResnetUnet))
======
While my view is by no means authoritative, I have some hangups about the previously mentioned solution to using ImageNet weights with 3-dimensional data.  For one, we're using the pretrained model's "understanding" of color channel data to resolve spatial features.  Our model will modify those parameters during the training process, but we can imagine, at the beginning of our training process, that the pathway through the model that resolves the features of a dog's face, for example, utilizes the information in red, green, and blue color channels very differently.  In comparison, three adjacent slices of a CT scan will be very similar, and we more likely than not want to resolve identical features within each slice.  An additional downside of placing spatial information in the channel dimension is that the convolutional kernels used in computer vision models look at every channel simultaneously, mixing their information to create each output channel.  However, it may be useful for the model to "think" about the features in each input slice separately for several layers, either to resolve them into more useful features before the information is eventually mixed or to increase the quality and/or amount of information communicated to some other part of the model (across a skip connection, perhaps).

ResNet
------
[ResNets](https://arxiv.org/abs/1512.03385) hardly need an introduction.  Suffice to say, while nowadays they face strong competition, ResNets are one of the most commonly chosen models for transfer learning tasks.  In my case, I chose them because I was familiar with the architecture and, more importantly, the [torch source code](https://pytorch.org/vision/0.8/_modules/torchvision/models/resnet.html) was relatively easy to understand and modify.  ResNets are typically trained on 224x224x3 RGB images, and often come pretrained on ImageNet.

My Changes
------
My goal is to adapt PyTorch's ResNet constructor to create a model I can use with 3-dimensional data.  More precisely, I will create a 2.5-dimensional model that I can use to analyze a collection of adjacent slices from a CT scan.  In order to do this, I take advantage of some straightforward geometry.  The "shape" of the tensors used by a 2-dimensional kernel (embedded in 3-d space) and a depth 1 3-dimensional kernel are, essentially, identical.  With the constraint that every kernel has depth 1, we can load ImageNet-pretrained ResNet parameters into a "3-d ResNet" model that has replaced every 2-d convolutional (or batch normalization, etc.) layer with its 3d equivalent using only minor code modifications.  Specifically, we only need to unsqueeze the depth dimension of the pretrained parameters.

To take care of the differences between greyscale and RGB images, I follow the 2-d procedure of duplicating my slice data into 3 channels.  This allows me to take advantage of the full set of features the ResNet "sees" on every slice of my data.  While I can't display them here, the result of my tests was a model that performed quite well considering the limited data I had for it to learn from.  In addition, for reasons I don't entirely understand, the UNet constructor I built to utilize the 3-d ResNets also used significantly less GPU memory than similar, 2-d models built using other frameworks, even when using some of the larger ResNets.

Though I used ResNet for my model, this process should be generalize fairly easily, and my future plans include applying it to other networks.

Limitations of this Process
------
Before ending this post it's worth talking about the limitations behind my decision to create a 2.5-d model instead of a truly 3-d model.  First, the key difference between 2.5-d and 3-d models is that the data is treated as if it were separate 2-d slices for most of the network, until at some point (e.g. the center of my UNet) we collapse the slices along the third dimension and allow the model to look at multiple slices simultaneously.  There are a couple reasons to do this.  For one, the resolution of my data in the X-Y dimensions is much higher than in the Z dimension, so there is certainly a much richer set of features available in that plane (See also: [EfficientNet's Discussion](https://arxiv.org/pdf/1905.11946.pdf)) and therefore some advantage to allowing the ResNet to process each slice independently.

More significantly, a true 3-d model precludes us from using a straightforward transfer learning process.  While 2-d kernels and depth 1 3-d kernels are essentially identical, increasing the depth of the 3-d kernel introduces ambiguity.  In analogy, consider a circle on a sheet of paper.  When asked for the 3-d interpretation of that circle, one might suggest a sphere, a disk, a cone, or any variation of those or several other shapes.  There is no well-defined, singular answer.  Similarly, there are any number of variations one might choose for the parameters loaded from the 2-d network when converting them to be truly 3-d, and there is no guarantee that choice is the best place to start or that it will even start the model in a learnable area of the loss topology.  While the latter problem is unlikely, a very good, perhaps unrealistically good, understanding of each layer of the pretrained network is needed to make an educated guess.
